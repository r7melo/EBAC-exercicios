{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e350f8fc",
   "metadata": {},
   "source": [
    "# Mod 24 Tarefa 02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0113bba-23d2-43af-81d5-c90c9f7e5fb3",
   "metadata": {},
   "source": [
    "1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "R: \n",
    "- AdaBoost é uma floresta de stumps e o GBM uma floresta de árvores;\n",
    "- O primeiro passo do GBM é a média do Y;\n",
    "- No GBM as ávores são criadas a partir do residuo, ou seja no final iremos criar a melhor árvore que explica o residuo;\n",
    "- No GBM todas as respostas tem o mesmo peso;\n",
    "- No GBM no final de cada resposta há sempre um mutiliplador em comum chamado ``eta`` (learning rate);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8797a2-99e2-47e1-ae53-fa61a31e1e91",
   "metadata": {},
   "source": [
    "2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47374e59-e28b-40dc-9d0f-380475be645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fa8a2-c38a-4ba1-a8af-7f9bf83513f9",
   "metadata": {},
   "source": [
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "- min_samples_leaf: O número mínimo de amostras necessárias para estar em um nó folha;\n",
    "- learning_rate: A taxa de aprendizado reduz a contribuição de cada árvore em learning_rate;\n",
    "- n_estimators: O número de estágios de boosting a serem executados;\n",
    "- subsample: A fração de amostras a ser usada para ajustar os aprendizes base individuais;\n",
    "- min_samples_split: O número mínimo de amostras necessárias para dividir um nó interno:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca187cbd-f8e0-4118-ab43-b1adb404b00a",
   "metadata": {},
   "source": [
    "4. (Opcional) Utilize o GridSearch para encontrar os\n",
    "melhores hyperparametros para o conjunto de dados\n",
    "do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1f6e15-a3e8-4515-a6b2-d5762e3e9a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'learning_rate': 0.1, 'min_samples_leaf': 20, 'n_estimators': 600}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9142"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "parametros = {\n",
    "    'n_estimators': [100, 300, 600],\n",
    "    'min_samples_leaf': [2, 10, 20],\n",
    "    'learning_rate': [0.04, 0.06, .1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=gb,\n",
    "                    param_grid=parametros,\n",
    "                    scoring='roc_auc',\n",
    "                    verbose=False,\n",
    "                    cv=2)\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "\n",
    "best_params = grid.best_params_\n",
    "print(f'Melhores parâmetros: {best_params}')\n",
    "\n",
    "\n",
    "score = grid.best_estimator_.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a81d1-e382-47cb-984f-ea37d3ed9e07",
   "metadata": {},
   "source": [
    "5. Acessando o artigo do Jerome Friedman (Stochastic) e\n",
    "pensando no nome dado ao Stochastic GBM, qual é a\n",
    "maior diferença entre os dois algoritmos?\n",
    "\n",
    "R: \n",
    "\n",
    "O GBM Utiliza todo o conjunto de dados para treinar cada árvore de decisão em cada iteração. Já o SGB Utiliza uma amostra aleatória dos dados para treinar cada árvore de decisão, reduzindo o overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
