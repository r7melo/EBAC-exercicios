{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e350f8fc",
   "metadata": {},
   "source": [
    "# Mod 24 Tarefa 02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0113bba-23d2-43af-81d5-c90c9f7e5fb3",
   "metadata": {},
   "source": [
    "1. Cite 5 diferenças entre o AdaBoost e o GBM.\n",
    "\n",
    "R: \n",
    "- AdaBoost é uma floresta de stumps e o GBM uma floresta de árvores;\n",
    "- O primeiro passo do GBM é a média do Y;\n",
    "- No GBM as ávores são criadas a partir do residuo, ou seja no final iremos criar a melhor árvore que explica o residuo;\n",
    "- No GBM todas as respostas tem o mesmo peso;\n",
    "- No GBM no final de cada resposta há sempre um mutiliplador em comum chamado ``eta`` (learning rate);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8797a2-99e2-47e1-ae53-fa61a31e1e91",
   "metadata": {},
   "source": [
    "2. Acesse o link Scikit-learn – GBM, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47374e59-e28b-40dc-9d0f-380475be645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fa8a2-c38a-4ba1-a8af-7f9bf83513f9",
   "metadata": {},
   "source": [
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "- min_samples_leaf: O número mínimo de amostras necessárias para estar em um nó folha;\n",
    "- learning_rate: A taxa de aprendizado reduz a contribuição de cada árvore em learning_rate;\n",
    "- n_estimators: O número de estágios de boosting a serem executados;\n",
    "- subsample: A fração de amostras a ser usada para ajustar os aprendizes base individuais;\n",
    "- min_samples_split: O número mínimo de amostras necessárias para dividir um nó interno:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca187cbd-f8e0-4118-ab43-b1adb404b00a",
   "metadata": {},
   "source": [
    "4. (Opcional) Utilize o GridSearch para encontrar os\n",
    "melhores hyperparametros para o conjunto de dados\n",
    "do exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1f6e15-a3e8-4515-a6b2-d5762e3e9a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 5,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    random_state=0,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "parametros = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_depth': [2, 3],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=gb,\n",
    "    param_grid=parametros,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train.ravel())\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a81d1-e382-47cb-984f-ea37d3ed9e07",
   "metadata": {},
   "source": [
    "5. Acessando o artigo do Jerome Friedman (Stochastic) e\n",
    "pensando no nome dado ao Stochastic GBM, qual é a\n",
    "maior diferença entre os dois algoritmos?\n",
    "\n",
    "R: \n",
    "\n",
    "O GBM Utiliza todo o conjunto de dados para treinar cada árvore de decisão em cada iteração. Já o SGB Utiliza uma amostra aleatória dos dados para treinar cada árvore de decisão, reduzindo o overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad650ec-4924-49e9-81d0-ef83266e13c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
